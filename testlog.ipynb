{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MSwwTJs9C32"
   },
   "outputs": [],
   "source": "#!pip install transformers datasets scikit-learn umap-learn --quiet"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaForMaskedLM,\n",
    "    LongformerTokenizer, LongformerForMaskedLM,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "# Choose dataset: Options = 'Thunderbird', 'HDFS', 'BGL'\n",
    "dataset_name = 'Thunderbird'\n",
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == 'Thunderbird':\n",
    "        !rm -rf loghub\n",
    "        !git clone https://github.com/logpai/loghub.git\n",
    "        paragraphs = []\n",
    "        current_paragraph = []\n",
    "        lines_per_paragraph = 30\n",
    "        log_file = \"loghub/Thunderbird/Thunderbird_2k.log\"\n",
    "\n",
    "        with open(log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    current_paragraph.append(line)\n",
    "                if len(current_paragraph) >= lines_per_paragraph:\n",
    "                    paragraphs.append(' '.join(current_paragraph))\n",
    "                    current_paragraph = []\n",
    "        if current_paragraph:\n",
    "            paragraphs.append(' '.join(current_paragraph))\n",
    "        paragraphs = np.array(paragraphs)\n",
    "\n",
    "        random.seed(42)\n",
    "        anomaly_indices = np.random.choice(len(paragraphs), size=int(0.2 * len(paragraphs)), replace=False)\n",
    "        normal_indices = np.array(list(set(range(len(paragraphs))) - set(anomaly_indices)))\n",
    "\n",
    "        normal_paragraphs = paragraphs[normal_indices]\n",
    "        anomalous_paragraphs = paragraphs[anomaly_indices]\n",
    "\n",
    "        return normal_paragraphs, anomalous_paragraphs\n",
    "\n",
    "    elif dataset_name == 'HDFS':\n",
    "        !rm -rf loghub\n",
    "        !git clone https://github.com/logpai/loghub.git\n",
    "        df = pd.read_csv('loghub/HDFS/HDFS.log_structured.csv')\n",
    "        block_labels = pd.read_csv('loghub/HDFS/anomaly_label.csv')\n",
    "\n",
    "        block_to_logs = {}\n",
    "        for _, row in df.iterrows():\n",
    "            blk = row['BlockId']\n",
    "            block_to_logs.setdefault(blk, []).append(row['Content'])\n",
    "\n",
    "        normal_paragraphs, anomalous_paragraphs = [], []\n",
    "\n",
    "        for blk, logs in block_to_logs.items():\n",
    "            paragraph = ' '.join(logs)\n",
    "            label = block_labels.loc[block_labels['BlockId'] == blk, 'Label'].values[0]\n",
    "            if label == 'Anomaly':\n",
    "                anomalous_paragraphs.append(paragraph)\n",
    "            else:\n",
    "                normal_paragraphs.append(paragraph)\n",
    "\n",
    "        return np.array(normal_paragraphs), np.array(anomalous_paragraphs)\n",
    "\n",
    "    elif dataset_name == 'BGL':\n",
    "        !rm -rf loghub\n",
    "        !git clone https://github.com/logpai/loghub.git\n",
    "        df = pd.read_csv('loghub/BGL/BGL.log_structured.csv')\n",
    "        paragraphs = []\n",
    "        current_paragraph = []\n",
    "        window_size = 30  # 30 seconds\n",
    "\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df = df.sort_values(by='Timestamp')\n",
    "        start_time = df['Timestamp'].iloc[0]\n",
    "\n",
    "        df['window_id'] = ((df['Timestamp'] - start_time).dt.total_seconds() // window_size).astype(int)\n",
    "        grouped = df.groupby('window_id')\n",
    "\n",
    "        for window_id, group in grouped:\n",
    "            content = ' '.join(group['Content'])\n",
    "            paragraphs.append((content, group['Label'].iloc[0]))\n",
    "\n",
    "        normal_paragraphs = [p[0] for p in paragraphs if p[1] == '-']\n",
    "        anomalous_paragraphs = [p[0] for p in paragraphs if p[1] != '-']\n",
    "\n",
    "        return np.array(normal_paragraphs), np.array(anomalous_paragraphs)\n",
    "\n",
    "normal_paragraphs, anomalous_paragraphs = load_dataset(dataset_name)\n",
    "print(f\"✅ Loaded {dataset_name}: {len(normal_paragraphs)} normal, {len(anomalous_paragraphs)} anomalies.\")\n"
   ],
   "metadata": {
    "id": "fEXcDLqI9GOX",
    "ExecuteTime": {
     "end_time": "2025-04-28T13:49:21.476423Z",
     "start_time": "2025-04-28T13:49:13.446989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'loghub'...\r\n",
      "remote: Enumerating objects: 575, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (171/171), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (38/38), done.\u001B[K\r\n",
      "remote: Total 575 (delta 145), reused 134 (delta 133), pack-reused 404 (from 2)\u001B[K(478/575), 5.60 MiB | 3.73 MiB/sReceiving objects:  88% (506/575), 7.24 MiB | 3.61 MiB/s\r\n",
      "Receiving objects: 100% (575/575), 7.27 MiB | 3.59 MiB/s, done.\r\n",
      "Resolving deltas: 100% (266/266), done.\r\n",
      "✅ Loaded BGL: 1 normal, 0 anomalies.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "class LogFiTDataset(Dataset):\n",
    "    def __init__(self, paragraphs, tokenizer, max_length=512, mask_prob_sentence=0.5, mask_prob_token=0.8):\n",
    "        self.paragraphs = paragraphs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_prob_sentence = mask_prob_sentence\n",
    "        self.mask_prob_token = mask_prob_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paragraphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.paragraphs[idx]\n",
    "        sentences = text.split('. ')\n",
    "        masked_sentences = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            if random.random() < self.mask_prob_sentence:\n",
    "                tokenized = self.tokenizer.tokenize(sent)\n",
    "                masked_tokens = []\n",
    "                for token in tokenized:\n",
    "                    if random.random() < self.mask_prob_token:\n",
    "                        masked_tokens.append(self.tokenizer.mask_token)\n",
    "                    else:\n",
    "                        masked_tokens.append(token)\n",
    "                masked_sent = self.tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "                masked_sentences.append(masked_sent)\n",
    "            else:\n",
    "                masked_sentences.append(sent)\n",
    "\n",
    "        masked_text = '. '.join(masked_sentences)\n",
    "\n",
    "        encoding = self.tokenizer(masked_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        labels = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': labels['input_ids'].squeeze(0)\n",
    "        }\n"
   ],
   "metadata": {
    "id": "kF3v60Rd9TEW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def top_k_accuracy(logits, labels, k=5):\n",
    "    top_k_preds = logits.topk(k, dim=-1).indices\n",
    "    labels = labels.unsqueeze(-1).expand_as(top_k_preds)\n",
    "    correct = (top_k_preds == labels).any(dim=-1)\n",
    "    return correct\n",
    "\n",
    "def detect_anomalies(model, paragraphs, tokenizer, k=5, threshold=0.6, batch_size=4):\n",
    "    model.eval()\n",
    "    anomalies = []\n",
    "\n",
    "    dataset = LogFiTDataset(paragraphs, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        acc = top_k_accuracy(logits, labels, k=k)\n",
    "        mask = labels != tokenizer.pad_token_id\n",
    "        acc = acc * mask\n",
    "        paragraph_accuracy = acc.sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        anomaly_flags = (paragraph_accuracy < threshold).long().tolist()\n",
    "        anomalies.extend(anomaly_flags)\n",
    "\n",
    "    return anomalies\n"
   ],
   "metadata": {
    "id": "30VyYRKZ9Vs0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Decide RoBERTa vs Longformer based on token length\n",
    "sample_paragraphs = random.sample(list(normal_paragraphs), min(10, len(normal_paragraphs)))\n",
    "token_counts = []\n",
    "\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer_longformer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "for paragraph in sample_paragraphs:\n",
    "    token_counts.append(len(tokenizer_roberta.tokenize(paragraph)))\n",
    "\n",
    "avg_tokens = np.mean(token_counts)\n",
    "print(f\"Average tokens per paragraph: {avg_tokens:.2f}\")\n",
    "\n",
    "if avg_tokens > 512:\n",
    "    print(\"✅ Using Longformer for this dataset.\")\n",
    "    tokenizer = tokenizer_longformer\n",
    "    base_model_name = 'allenai/longformer-base-4096'\n",
    "    model_class = LongformerForMaskedLM\n",
    "else:\n",
    "    print(\"✅ Using RoBERTa for this dataset.\")\n",
    "    tokenizer = tokenizer_roberta\n",
    "    base_model_name = 'roberta-base'\n",
    "    model_class = RobertaForMaskedLM\n"
   ],
   "metadata": {
    "id": "wHKeQlTC9aOm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cross-validation settings\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(normal_paragraphs)):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    train_normals = normal_paragraphs[train_idx][:5000] if len(train_idx) >= 5000 else normal_paragraphs[train_idx]\n",
    "    val_normals = normal_paragraphs[train_idx][5000:6000] if len(train_idx) >= 6000 else normal_paragraphs[train_idx][-1000:]\n",
    "    test_normals = normal_paragraphs[test_idx][:5000] if len(test_idx) >= 5000 else normal_paragraphs[test_idx]\n",
    "\n",
    "    val_anomalies = anomalous_paragraphs[:1000]\n",
    "    test_anomalies = anomalous_paragraphs[1000:2000]\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = LogFiTDataset(train_normals, tokenizer)\n",
    "\n",
    "    # Load model\n",
    "    model = model_class.from_pretrained(base_model_name)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Training settings\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./logfit_model_fold{fold+1}\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3 if dataset_name == 'Thunderbird' else 5,\n",
    "        per_device_train_batch_size=4,  # Lower batch for Colab\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        logging_dir=f\"./logfit_logs_fold{fold+1}\",\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Validation set\n",
    "    val_paragraphs = np.concatenate([val_normals, val_anomalies])\n",
    "    val_labels = [0]*len(val_normals) + [1]*len(val_anomalies)\n",
    "\n",
    "    # Automatic threshold tuning\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    for threshold in np.arange(0.5, 0.96, 0.05):\n",
    "        preds_val = detect_anomalies(model, val_paragraphs, tokenizer, threshold=threshold, batch_size=4)\n",
    "        precision = precision_score(val_labels, preds_val)\n",
    "        recall = recall_score(val_labels, preds_val)\n",
    "        f1 = f1_score(val_labels, preds_val)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"✅ Best threshold on validation: {best_threshold:.2f} with F1: {best_f1:.4f}\")\n",
    "\n",
    "    # Final Test\n",
    "    test_paragraphs = np.concatenate([test_normals, test_anomalies])\n",
    "    test_labels = [0]*len(test_normals) + [1]*len(test_anomalies)\n",
    "\n",
    "    preds_test = detect_anomalies(model, test_paragraphs, tokenizer, threshold=best_threshold, batch_size=4)\n",
    "\n",
    "    precision = precision_score(test_labels, preds_test)\n",
    "    recall = recall_score(test_labels, preds_test)\n",
    "    f1 = f1_score(test_labels, preds_test)\n",
    "\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"[Test] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del model\n",
    "    del trainer\n",
    "    del train_dataset\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Report final 5-Fold Results\n",
    "print(\"\\n=== Final 5-Fold Results ===\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ],
   "metadata": {
    "id": "H7loQLPN9iVp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_precision_recall_curve(y_true, y_scores, title='Precision-Recall Curve'):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.', label=f'AUC = {pr_auc:.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "r-RcP6349xpP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.colorbar()\n",
    "\n",
    "    classes = ['Normal', 'Anomaly']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "fBuibwbS9zrI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# After testing on test_paragraphs\n",
    "# Plot Precision-Recall curve\n",
    "plot_precision_recall_curve(test_labels, preds_test, title=f'Precision-Recall Fold {fold+1}')\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plot_confusion_matrix(test_labels, preds_test, title=f'Confusion Matrix Fold {fold+1}')\n"
   ],
   "metadata": {
    "id": "SxfM6LBX92E3"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
